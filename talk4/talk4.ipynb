{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will demonstrate how to implement the Gradient Descent and the Stochastic Gradient Descent for a simple least-squared minimisation problem (fitting a line to some data points). We compare the performance against the closed-form solution, and we compare how fast both methods converge.\n",
    "\n",
    "\n",
    "<b>Note:</b> I have <b>intentionally bombarded</b> this notebook with comments to minimise any confusion. If you find them annoying, as I do, just delete them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing purposes, we import 1D data points $\\{(x_i,y_i)\\}$ for $i=1, \\cdots N$, using `make_regression`. This gives data points behaving linearly with some artificial noise: $$Y= C X+\\text{noise}.$$ Then we plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(\n",
    "    n_samples=300, n_features=1, n_targets=1, noise=80, random_state=0\n",
    ")\n",
    "\n",
    "# make_regression returns NumPy arrays. Turn them into JAX arrays\n",
    "X = jnp.array(X)\n",
    "y = jnp.array(y).reshape(\n",
    "    -1, 1\n",
    ")  # transfer to (rows, columns). (-1,1) gives column vector (1 column and -1 means figure out how many rows).\n",
    "\n",
    "# n_samples is the number of data points (N in the notebook)\n",
    "# n_features and n_targets: the dimensionality of the data points. Setting them both 1 means R x R\n",
    "# random state: if this is not specified, new random data will be generated with each execution\n",
    "# noise: how much random Gaussian noise is added to the Y values. Here, 80 is the standard deviation of this noise (e.g. if 0 you get a straight line).\n",
    "\n",
    "# plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.scatter(X, y, alpha=0.5)  # alpha = transparency of the data points colour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the least-squares problem. The objective function in this case is\n",
    "$$L(a,b) = \\dfrac{1}{2N} \\sum_{i=1}^N |ax_i+b-y_i|^2,$$\n",
    "where $a,b \\in \\mathbb{R}$ are the parameters to be determined, and $N =300$ in our example.\n",
    "This can be written in matrix form. Set $\\theta=(a,b)^\\top \\in \\mathbb{R}^2$, then we have\n",
    "$$ L(\\theta) = \\dfrac{1}{2} \\| A \\theta - y \\|^2_2 ,$$\n",
    "where $A \\in \\mathbb{R}^{N \\times 2}$ and $y \\in \\mathbb{R}^N$ are given by\n",
    "\n",
    "$$ A = \\begin{bmatrix} x_1 & 1 \\\\ x_2 & 1 \\\\ \\vdots & \\vdots \\\\ x_N & 1 \\end{bmatrix}, \\quad \\quad y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = jnp.column_stack([X, jnp.ones_like(X)])  # creates A = [[X] [1]]\n",
    "\n",
    "\n",
    "# the objective function = mean squared distance\n",
    "def loss(theta, A, y):\n",
    "    return jnp.mean(((A @ theta) - y) ** 2)\n",
    "\n",
    "\n",
    "# respect the order here. Differentiation is w.r.t first argument unless specified otherwise\n",
    "grad_loss = jax.grad(loss)  # argnums by default = 0. i.e. first argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The closed-form solution is given by\n",
    "$$\\theta = (A^\\top A)^{-1}(A^\\top y). $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# closed form of the solution (consult week 3 talk)\n",
    "theta_closed = jnp.linalg.inv(A.T @ A) @ (A.T @ y)\n",
    "print(theta_closed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the gradient descent method to find an approximate $\\theta^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent\n",
    "# an initial value\n",
    "theta_gd = jnp.array([[0.0, 0.0]]).T\n",
    "\n",
    "mu = jnp.min(jnp.linalg.eigvals(A.T @ A))\n",
    "delta = jnp.max(jnp.linalg.eigvals(A.T @ A))\n",
    "\n",
    "# jnp.linalg.eigvals always returns complex numbers\n",
    "\n",
    "alpha = 2 / (jnp.real(mu + delta))\n",
    "\n",
    "\n",
    "gd_losses = []  # to keep track of the loss at each iteration\n",
    "\n",
    "\n",
    "# iterations of the gradient descent\n",
    "for k in range(1, 674):\n",
    "    grad_gd = grad_loss(theta_gd, A, y)\n",
    "\n",
    "    theta_gd -= alpha * grad_gd  # theta_gd = theta_gd - alpha *g\n",
    "    gd_losses.append(loss(theta_gd, A, y))\n",
    "\n",
    "    if k % 100 == 0:\n",
    "        print(\n",
    "            f\"Loss function value at step {k}: {gd_losses[k-1]}\"\n",
    "        )  # k-1? because k starts at 1 but gd_losses index starts at 0.\n",
    "\n",
    "\n",
    "print(\"Using Gradient Descent, theta^* =\", theta_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the Gradient Descent solution against the closed-form solution for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting-up the plot\n",
    "plt.figure(figsize=(14, 8))\n",
    "x_plot = jnp.linspace(X.min(), X.max(), 100)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Predicted y\")\n",
    "\n",
    "# plot data points\n",
    "plt.scatter(\n",
    "    X, y, label=\"Data\", alpha=0.5\n",
    ")  # alpha = transparency of the data points color\n",
    "\n",
    "# create lines to be fitted\n",
    "y_gd = theta_gd[0, 0] * x_plot + theta_gd[1, 0]  # from GD solution\n",
    "y_exact = theta_closed[0, 0] * x_plot + theta_closed[1, 0]  # from closed-form solution\n",
    "\n",
    "# plotting the lines\n",
    "plt.plot(x_plot, y_gd, label=\"Gradient Descent\", color=\"red\")\n",
    "plt.plot(x_plot, y_exact, label=\"Closed-Form\", linestyle=\"--\", color=\"black\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we introduce the mini-batch gradient descent. In the following, epoch refers to a complete pass through the entire data set. This suggests that the performance of the model can be evaluated after each epoch. We thus calculate the loss function at the end of each epoch iteration. In each epoch, we choose a mini-batch, and use it to update the gradient, then update the required parameter $\\theta^*$ using this noisy gradient. After scanning through all of our data points, we are ready to start a new epoch. We can also observe the loss function values after each mini-batch iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-batch Gradient Descent\n",
    "batch_size = 63\n",
    "epochs = 135  # cycle through the entire data set 135 times\n",
    "theta_mbgd = jnp.array([[0.0, 0.0]]).T\n",
    "# record two kinds of losses: 1-error after each mini-batch iteration, 2-error after each epoch iteration\n",
    "mbgd_losses1 = []\n",
    "mbgd_losses2 = []\n",
    "\n",
    "np.random.seed(0)  # to reproduce the same random number each time you execute\n",
    "\n",
    "\n",
    "# iterations of the mini-batch gradient descent\n",
    "for epoch in range(epochs):\n",
    "    indices = np.random.permutation(\n",
    "        len(y)\n",
    "    )  # to generate a random permutation of size len(y)\n",
    "\n",
    "    # shuffle data to randomise the order\n",
    "    A_shuffled = A[\n",
    "        indices\n",
    "    ]  # to shuffle the rows of A according to the permutation \"indices\"\n",
    "    y_shuffled = y[indices]\n",
    "\n",
    "    batch_loss = []\n",
    "\n",
    "    for i in range(0, len(y), batch_size):\n",
    "        # if len(y) not divisible by batch_size, loop handles it by taking a smaller batch_size (i.e. last batch_size = len(y) mod batch_size)\n",
    "\n",
    "        # take a batch of size batch_size\n",
    "        A_batch = A_shuffled[\n",
    "            i : i + batch_size, :\n",
    "        ]  # take only row i until row i + batch_size. Take all columns\n",
    "        y_batch = y_shuffled[i : i + batch_size, :]\n",
    "\n",
    "        # to see the size of each batch taken (in this example we get 5 mini-batches of sizes: 63, 63, 63, 63 and 48)\n",
    "        batch_size_actual = A_batch.shape[0]\n",
    "        ##print(batch_size_actual)\n",
    "\n",
    "        # usual gradient descent but with the mini-batch\n",
    "        grad_batch = grad_loss(theta_mbgd, A_batch, y_batch)\n",
    "        theta_mbgd -= alpha * grad_batch\n",
    "\n",
    "        # keep track of loss at each mini-batch iteration\n",
    "        batch_loss.append(loss(theta_mbgd, A_batch, y_batch))\n",
    "\n",
    "    mbgd_losses1.extend(batch_loss)  # append for elements, extend for lists\n",
    "    mbgd_losses2.append(loss(theta_mbgd, A, y))\n",
    "\n",
    "\n",
    "print(\"Using Gradient Descent, theta^* =\", theta_mbgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the Mini-batch Gradient Descent solution against the closed-form and the Gradient Descent solutions for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting-up the plot\n",
    "plt.figure(figsize=(14, 8))\n",
    "x_plot = jnp.linspace(X.min(), X.max(), 100)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Predicted y\")\n",
    "\n",
    "# plot data points\n",
    "plt.scatter(X, y, label=\"Data\", alpha=0.5)\n",
    "\n",
    "# create the lines\n",
    "y_gd = theta_gd[0, 0] * x_plot + theta_gd[1, 0]  # GD\n",
    "y_exact = theta_closed[0, 0] * x_plot + theta_closed[1, 0]  # closed-form\n",
    "y_mbgd = theta_mbgd[0, 0] * x_plot + theta_mbgd[1, 0]  # mini-batch GD\n",
    "\n",
    "# plot the lines\n",
    "plt.plot(x_plot, y_gd, label=\"Gradient Descent\", color=\"red\")\n",
    "plt.plot(x_plot, y_exact, label=\"Closed-Form\", linestyle=\"--\", color=\"black\")\n",
    "plt.plot(x_plot, y_mbgd, label=\"mini-batch GD\", linestyle=\"-.\", color=\"green\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compare the decay of the loss function for the Gradient Descent and the Mini-batch Gradient Descent. For the latter, we plot the loss function values after each mini-bath iteration (in red), and after each epoch (in green)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convergence plot (decrease in loss function)\n",
    "\n",
    "# setting-up the plots\n",
    "plt.figure(figsize=(22, 11))\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "\n",
    "# for GD\n",
    "plt.plot(gd_losses, label=\"GD Loss\", color=\"black\")\n",
    "\n",
    "# for mini-batch GD after each mini-batch (mbgd_losses1)\n",
    "plt.plot(mbgd_losses1, label=\"mini-batch GD Loss_1\", alpha=0.5, color=\"red\")\n",
    "\n",
    "# for mini-batch GD after each epoch (mbgd_losses2)\n",
    "plt.plot(mbgd_losses2, label=\"mini-batch GD Loss_2\", color=\"green\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>More to explore (easier to harder):</b>\n",
    "* Accelerated Gradient Descent: try implementing Nesterov acceleration and see if it is indeed faster as the theory dictates;\n",
    "\n",
    "* Real Machine Learning Examples: try using these methods on an actual machine learning problem where the data can be more noisy, the objective function can be more complicated, or the learning rate is a non-constant;\n",
    "\n",
    "* User-Friendly Code: this code is constructed around our least-square example (objective function, learning rate, data generated...etc). How about giving the user the freedom of choosing?\n",
    "\n",
    "* Object-Oriented Programming: this is an advanced programming style that designs your code as objects. A good start would be to learn about `Class` in python, and then check [Object-Oriented Programming (OOP) in Python](https://realpython.com/python3-object-oriented-programming/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Useful online resources (Last accessed: 20 May 2025):</b>\n",
    "* [Guide to Gradient Descent Algorithm: A Comprehensive implementation in Python](https://machinelearningspace.com/a-comprehensive-guide-to-gradient-descent-algorithm/);\n",
    "\n",
    "* [Stochastic Gradient Descent Algorithm With Python and NumPy](https://realpython.com/gradient-descent-algorithm-python/);\n",
    "\n",
    "* [Stochastic Gradient Descent in Python: A Complete Guide for ML Optimization](https://www.datacamp.com/tutorial/stochastic-gradient-descent?dc_referrer=https%3A%2F%2Fwww.google.com%2F)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
